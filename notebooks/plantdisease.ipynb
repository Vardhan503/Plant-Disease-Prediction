{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cb1ef071",
      "metadata": {
        "id": "cb1ef071"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "import os\n",
        "import random\n",
        "from pathlib import Path\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "from PIL import Image\n",
        "import plotly.express as px\n",
        "from sklearn.model_selection import train_test_split\n",
        "from torchvision import transforms, datasets\n",
        "from torch.utils.data import DataLoader, WeightedRandomSampler, random_split\n",
        "from torch.utils.data.sampler import SubsetRandomSampler\n",
        "import torchvision\n",
        "import torch.optim as optim\n",
        "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
        "import shutil"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "pQWE80Uua3fs",
      "metadata": {
        "id": "pQWE80Uua3fs"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 1. Define paths\n",
        "zip_path_in_drive = '/content/drive/MyDrive/plantvillage dataset'\n",
        "local_zip_path = '/content/PlantVillage_local'\n",
        "\n",
        "# 2. Copy data from Drive to local runtime (Much faster IO)\n",
        "# Check if the destination directory already exists to prevent an error with shutil.copytree\n",
        "if not os.path.exists(local_zip_path):\n",
        "    print(f\"Copying data from Drive ({zip_path_in_drive}) to local VM ({local_zip_path})...\")\n",
        "    shutil.copytree(zip_path_in_drive, local_zip_path)\n",
        "    print(\"Copy complete.\")\n",
        "else:\n",
        "    print(f\"Local dataset directory '{local_zip_path}' already exists. Skipping copy.\")\n",
        "\n",
        "data_dir = f\"{local_zip_path}/color\""
      ],
      "metadata": {
        "id": "Ar5jdibL2sQs"
      },
      "id": "Ar5jdibL2sQs",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class_names = os.listdir(data_dir)\n",
        "class_counts = {}\n",
        "for class_name in class_names:\n",
        "  class_path = os.path.join(data_dir, class_name)\n",
        "  if os.path.isdir(class_path):\n",
        "    class_counts[class_name] = len(os.listdir(class_path))\n",
        "\n",
        "\n",
        "df = pd.DataFrame(list(class_counts.items()), columns=['Class', 'Count'])\n",
        "plt.figure(figsize=(20, 8))\n",
        "sns.barplot(x=\"Class\", y=\"Count\", data=df)\n",
        "plt.title('Class Distribution')\n",
        "plt.xticks(rotation=90)\n",
        "plt.show()\n",
        "\n",
        "print(f'Total number of images:{df.Count.sum()}')\n",
        "print(f'Min number of images in a Class:{df.Count.min()}')\n",
        "print(f'Max number of images in a Class:{df.Count.max()}')"
      ],
      "metadata": {
        "id": "2et-V-w7AEfL"
      },
      "id": "2et-V-w7AEfL",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def show_random_images(data_dir, num_images=12):\n",
        "  class_names = os.listdir(data_dir)\n",
        "  for class_name in range(num_images):\n",
        "    random_class = random.choice(class_names)\n",
        "    random_class_path = os.path.join(data_dir, random_class)\n",
        "    random_image = random.choice(os.listdir(random_class_path))\n",
        "    random_image_path = os.path.join(random_class_path, random_image)\n",
        "    image = Image.open(random_image_path)\n",
        "    plt.imshow(image)\n",
        "    plt.subplot(3, 4, class_name+1)\n",
        "    plt.title(f'Class: {random_class.split('___')[0]}')\n",
        "    plt.axis('off')\n",
        "    plt.show()\n",
        "\n",
        "show_random_images(data_dir)\n",
        "\n"
      ],
      "metadata": {
        "id": "tsCukG81DNyN"
      },
      "id": "tsCukG81DNyN",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "widths = []\n",
        "heights = []\n",
        "corrupt = []\n",
        "\n",
        "for dirpath, dirnames, filenames in os.walk(data_dir):\n",
        "  for filename in filenames:\n",
        "    if filename.endswith(('.jpg', '.png', 'JPG', 'jpeg', 'JPEG')):\n",
        "      try:\n",
        "        image_path = os.path.join(dirpath, filename)\n",
        "        with Image.open(image_path) as img:\n",
        "          widths.append(img.width)\n",
        "          heights.append(img.height)\n",
        "          img.verify()\n",
        "      except Exception as e:\n",
        "        corrupt.append(image_path)\n",
        "        print(f'Error processing {image_path}: {e}')\n",
        "\n",
        "print(f'Average width: {sum(widths)/len(widths)}')\n",
        "print(f'Average Heigth: {sum(heights)/len(heights)}')\n",
        "print(f'Total number of corrupt images: {len(corrupt)}')\n",
        "\n",
        "plt.figure(figsize=(8,6))\n",
        "plt.scatter(widths, heights, alpha=0.5)\n",
        "plt.title(\"Image Dimensions Scatter Plot\")\n",
        "plt.xlabel(\"Width\")\n",
        "plt.ylabel(\"Height\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "0ORzzJsTGZ_O"
      },
      "id": "0ORzzJsTGZ_O",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_mean_std(data_dir, sample_size = 1000):\n",
        "  all_files = []\n",
        "  for dirpath, dirnames, filenames in os.walk(data_dir):\n",
        "    for filename in filenames:\n",
        "      if filename.endswith(('.jpg', '.png', 'JPG', 'jpeg', 'JPEG')):\n",
        "        all_files.append(os.path.join(dirpath, filename))\n",
        "\n",
        "  random_files = random.sample(all_files, sample_size)\n",
        "  pixel_values = []\n",
        "  for f in random_files:\n",
        "      img = Image.open(f).convert('RGB')\n",
        "      img = img.resize((224, 224)) # Resize to what model sees\n",
        "      img_array = np.array(img) / 255.0 # Normalize to 0-1\n",
        "      pixel_values.append(img_array)\n",
        "\n",
        "  mean = np.mean(pixel_values, axis=(0, 1, 2))\n",
        "  std = np.std(pixel_values, axis=(0, 1, 2))\n",
        "\n",
        "  return mean, std\n",
        "\n",
        "custom_mean, custom_std = get_mean_std(data_dir)\n",
        "print(f\"Custom Mean: {custom_mean}\")\n",
        "print(f\"Custom Std: {custom_std}\")\n"
      ],
      "metadata": {
        "id": "1bjJqnJ3QorR"
      },
      "id": "1bjJqnJ3QorR",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_transforms = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),\n",
        "    transforms.RandomAffine(\n",
        "        degrees=40,          # rotation_range=40\n",
        "        translate=(0.2, 0.2),# width/height_shift_range=0.2\n",
        "        shear=0.2,           # shear_range=0.2\n",
        "        scale=(0.8, 1.2)     # zoom_range=0.2 (0.8 to 1.2)\n",
        "    ),\n",
        "    transforms.RandomHorizontalFlip(),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
        "])\n",
        "\n",
        "val_transforms = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
        "])\n"
      ],
      "metadata": {
        "id": "eDJtRr3m4lS_"
      },
      "id": "eDJtRr3m4lS_",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "BATCH_SIZE = 32\n",
        "dataset = datasets.ImageFolder(root=data_dir)\n",
        "targets = dataset.targets\n",
        "\n",
        "train_idx, val_idx = train_test_split(\n",
        "    np.arange(len(targets)),\n",
        "    test_size=0.2,\n",
        "    shuffle=True,\n",
        "    stratify=targets\n",
        ")\n",
        "\n",
        "class_counts = np.bincount(targets)\n",
        "class_weights = 1. / class_counts\n",
        "sample_weights = class_weights[targets]\n",
        "sample_weights = torch.from_numpy(sample_weights)\n",
        "train_weights = sample_weights[train_idx]\n",
        "\n",
        "train_sampler = WeightedRandomSampler(\n",
        "    weights=train_weights,\n",
        "    replacement=True,\n",
        "    num_samples=len(train_idx)\n",
        ")\n",
        "\n",
        "val_sampler = SubsetRandomSampler(val_idx)\n",
        "\n",
        "train_dataset = datasets.ImageFolder(root=data_dir, transform=train_transforms)\n",
        "test_dataset = datasets.ImageFolder(root=data_dir, transform=val_transforms)\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, sampler=train_sampler,num_workers=2, pin_memory=True )\n",
        "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, sampler=val_sampler, num_workers=2, pin_memory=True)\n",
        "\n",
        "print(f\"Loaders ready.\")\n",
        "print(f\"Training on {len(train_idx)} images.\")\n",
        "print(f\"Validating on {len(val_idx)} images.\")\n",
        "print(f\"Classes: {len(dataset.classes)}\")\n",
        "print(f\"Classes: {dataset.classes}\")"
      ],
      "metadata": {
        "id": "TNI7Zr6h4g1D"
      },
      "id": "TNI7Zr6h4g1D",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Training Model on Custom CNN"
      ],
      "metadata": {
        "id": "_5W4sM2Dp7VS"
      },
      "id": "_5W4sM2Dp7VS"
    },
    {
      "cell_type": "code",
      "source": [
        "class PlantDiseaseCNN(nn.Module):\n",
        "    def __init__(self, num_classes=38):\n",
        "        super(PlantDiseaseCNN, self).__init__()\n",
        "\n",
        "        # Block 1\n",
        "        # TF: Conv2D(32, (3,3)) -> Conv2D(64, (3,3)) -> MaxPool\n",
        "        self.block1 = nn.Sequential(\n",
        "            nn.Conv2d(in_channels=3, out_channels=32, kernel_size=3, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.BatchNorm2d(32), # Batch Norm helps convergence (optional but recommended)\n",
        "            nn.Conv2d(32, 64, kernel_size=3, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.BatchNorm2d(64),\n",
        "            nn.MaxPool2d(kernel_size=2, stride=2) # Image becomes 112x112\n",
        "        )\n",
        "\n",
        "        # Block 2\n",
        "        # TF: Conv2D(64, (3,3)) -> Conv2D(64, (3,3)) -> MaxPool\n",
        "        self.block2 = nn.Sequential(\n",
        "            nn.Conv2d(64, 64, kernel_size=3, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.BatchNorm2d(64),\n",
        "            nn.Conv2d(64, 64, kernel_size=3, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.BatchNorm2d(64),\n",
        "            nn.MaxPool2d(kernel_size=2, stride=2) # Image becomes 56x56\n",
        "        )\n",
        "\n",
        "        # Block 3\n",
        "        # TF: Conv2D(128, (3,3)) -> Conv2D(128, (3,3)) -> MaxPool\n",
        "        self.block3 = nn.Sequential(\n",
        "            nn.Conv2d(64, 128, kernel_size=3, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.BatchNorm2d(128),\n",
        "            nn.Conv2d(128, 128, kernel_size=3, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.BatchNorm2d(128),\n",
        "            nn.MaxPool2d(kernel_size=2, stride=2), # Image becomes 28x28\n",
        "            nn.Dropout(0.2)\n",
        "        )\n",
        "\n",
        "        # Classifier Head\n",
        "        self.flatten = nn.Flatten()\n",
        "\n",
        "        # Math: 128 channels * 28 width * 28 height = 100,352 inputs\n",
        "        self.fc_layers = nn.Sequential(\n",
        "            nn.Linear(128 * 28 * 28, 256),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(256, 128),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.5),\n",
        "            nn.Linear(128, num_classes)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.block1(x)\n",
        "        x = self.block2(x)\n",
        "        x = self.block3(x)\n",
        "        x = self.flatten(x)\n",
        "        x = self.fc_layers(x)\n",
        "        return x"
      ],
      "metadata": {
        "id": "WdwBPPGX39-T"
      },
      "id": "WdwBPPGX39-T",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "devcice = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "model = PlantDiseaseCNN(num_classes=38).to(device)\n",
        "\n",
        "# 2. Setup Optimizer & Scheduler\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "scheduler = ReduceLROnPlateau(optimizer, mode='min', factor=0.3, patience=2)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "# 3. Training Loop\n",
        "NUM_EPOCHS = 20"
      ],
      "metadata": {
        "id": "gE0KHt2_8-hK"
      },
      "id": "gE0KHt2_8-hK",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for epoch in range(NUM_EPOCHS):\n",
        "    model.train()\n",
        "    train_loss = 0\n",
        "    train_correct = 0\n",
        "    total_train = 0\n",
        "\n",
        "    for images, labels in train_loader:\n",
        "        images, labels = images.to(device), labels.to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(images)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        train_loss += loss.item()\n",
        "        _, preds = torch.max(outputs, 1)\n",
        "        train_correct += (preds == labels).sum().item()\n",
        "        total_train += labels.size(0)\n",
        "\n",
        "    avg_train_loss = train_loss / len(train_loader)\n",
        "    train_acc = train_correct / total_train\n",
        "\n",
        "    model.eval()\n",
        "    val_loss = 0.0\n",
        "    val_correct = 0\n",
        "    total_val = 0\n",
        "\n",
        "    with torch.inference_mode():\n",
        "        for images, labels in test_loader:\n",
        "            images, labels = images.to(device), labels.to(device)\n",
        "            outputs = model(images)\n",
        "            loss = criterion(outputs, labels)\n",
        "\n",
        "            val_loss += loss.item()\n",
        "            _, preds = torch.max(outputs, 1)\n",
        "            val_correct += (preds == labels).sum().item()\n",
        "            total_val += labels.size(0)\n",
        "\n",
        "    avg_val_loss = val_loss / len(test_loader)\n",
        "    val_acc = val_correct / total_val\n",
        "\n",
        "    scheduler.step(avg_val_loss)\n",
        "\n",
        "    print(f\"Epoch {epoch+1}/{NUM_EPOCHS} | \"\n",
        "          f\"Train Loss: {avg_train_loss:.4f} Acc: {train_acc:.4f} | \"\n",
        "          f\"Val Loss: {avg_val_loss:.4f} Acc: {val_acc:.4f}\")\n"
      ],
      "metadata": {
        "id": "4uevj6yF3Ckh"
      },
      "id": "4uevj6yF3Ckh",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "save_path = \"/content/drive/MyDrive/modelss/plant_disease_custom_cnn.pth\"\n",
        "os.makedirs(os.path.dirname(save_path), exist_ok=True)\n",
        "torch.save(model.state_dict(), save_path)"
      ],
      "metadata": {
        "id": "SRaF6aD89UXA"
      },
      "id": "SRaF6aD89UXA",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Training Model on ResNet"
      ],
      "metadata": {
        "id": "TBOJmV10pvse"
      },
      "id": "TBOJmV10pvse"
    },
    {
      "cell_type": "code",
      "source": [
        "BATCH_SIZE = 32\n",
        "\n",
        "norm_mean = [0.485, 0.456, 0.406]\n",
        "norm_std = [0.229, 0.224, 0.225]\n",
        "\n",
        "train_transforms = transforms.Compose([\n",
        "    transforms.Resize((224,224)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=norm_mean, std=norm_std),\n",
        "    transforms.RandomHorizontalFlip(),\n",
        "    transforms.RandomRotation(10)\n",
        "])\n",
        "\n",
        "val_transforms = transforms.Compose([\n",
        "    transforms.Resize((224,224)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=norm_mean, std=norm_std)\n",
        "])\n"
      ],
      "metadata": {
        "id": "blU2X4HuaIZt"
      },
      "id": "blU2X4HuaIZt",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset = datasets.ImageFolder(root=data_dir)\n",
        "targets = dataset.targets\n",
        "\n",
        "train_idx, val_idx = train_test_split(\n",
        "    np.arange(len(targets)),\n",
        "    test_size=0.2,\n",
        "    shuffle=True,\n",
        "    stratify=targets\n",
        ")\n",
        "\n",
        "class_counts = np.bincount(targets)\n",
        "class_weights = 1. / class_counts\n",
        "sample_weights = class_weights[targets]\n",
        "sample_weights = torch.from_numpy(sample_weights)\n",
        "train_weights = sample_weights[train_idx]\n",
        "\n",
        "train_sampler = WeightedRandomSampler(\n",
        "    weights=train_weights,\n",
        "    replacement=True,\n",
        "    num_samples=len(train_idx)\n",
        ")\n",
        "\n",
        "val_sampler = SubsetRandomSampler(val_idx)\n",
        "\n",
        "train_dataset = datasets.ImageFolder(root=data_dir, transform=train_transforms)\n",
        "test_dataset = datasets.ImageFolder(root=data_dir, transform=val_transforms)\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, sampler=train_sampler)\n",
        "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, sampler=val_sampler)\n",
        "\n",
        "print(f\"Loaders ready.\")\n",
        "print(f\"Training on {len(train_idx)} images.\")\n",
        "print(f\"Validating on {len(val_idx)} images.\")\n",
        "print(f\"Classes: {len(dataset.classes)}\")"
      ],
      "metadata": {
        "id": "sgOALRVAtT2F"
      },
      "id": "sgOALRVAtT2F",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "weights = torchvision.models.ResNet18_Weights.DEFAULT"
      ],
      "metadata": {
        "id": "1Wk6Fk4krkdL"
      },
      "id": "1Wk6Fk4krkdL",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = torchvision.models.resnet18(weights=weights)\n",
        "for param in model.parameters():\n",
        "  param.requires_grad = False\n",
        "\n",
        "model.fc = nn.Linear(in_features=model.fc.in_features, out_features=38, bias=True)\n",
        "\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "model.to(device)\n",
        "\n",
        "loss_fn = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(model.fc.parameters(), lr=0.01)"
      ],
      "metadata": {
        "id": "UIJ9OVJp1yhO"
      },
      "id": "UIJ9OVJp1yhO",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "NUM_EPOCHS = 5\n",
        "\n",
        "for epoch in range(NUM_EPOCHS):\n",
        "  train_loss = 0\n",
        "  train_correct = 0 # To track accuracy\n",
        "  total_samples = 0\n",
        "  model.train()\n",
        "  for images, labels in train_loader:\n",
        "    images, labels = images.to(device), labels.to(device)\n",
        "    y_pred = model(images)\n",
        "    loss = loss_fn(y_pred, labels)\n",
        "    train_loss+=loss.item()\n",
        "    _, preds = torch.max(y_pred, dim=1)\n",
        "    train_correct += (preds == labels).sum().item()\n",
        "    total_samples += labels.size(0)\n",
        "    optimizer.zero_grad()\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    # Calculate average loss/acc for the epoch\n",
        "    avg_train_loss = train_loss / len(train_loader)\n",
        "    avg_train_acc = train_correct / total_samples\n",
        "  model.eval()\n",
        "  with torch.inference_mode():\n",
        "    test_loss = 0.0\n",
        "    test_correct = 0\n",
        "    total_samples = 0\n",
        "    model.eval()\n",
        "    for images, labels in test_loader:\n",
        "      images, labels = images.to(device), labels.to(device)\n",
        "      test_pred = model(images)\n",
        "      loss=loss_fn(test_pred, labels)\n",
        "      test_loss+=loss.item()\n",
        "      _, preds = torch.max(test_pred, dim=1)\n",
        "      test_correct += (preds == labels).sum().item()\n",
        "      total_samples += labels.size(0)\n",
        "  avg_test_loss = test_loss / len(test_loader)\n",
        "  avg_test_acc = test_correct / total_samples\n",
        "  print(f\"Epoch [{epoch+1}/{NUM_EPOCHS}] \"\n",
        "          f\"Train Loss: {avg_train_loss:.4f} | Train Acc: {avg_train_acc*100:.2f}% | \"\n",
        "          f\"Val Loss: {avg_test_loss:.4f} | Val Acc: {avg_test_acc*100:.2f}%\")\n",
        "\n",
        "torch.save(model.state_dict(), \"plant_disease_model.pth\")\n",
        "print(\"Model saved successfully.\")"
      ],
      "metadata": {
        "id": "VhGzSy5sCZzP"
      },
      "id": "VhGzSy5sCZzP",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "save_path = \"/content/drive/MyDrive/modelss/plant_disease_resnet.pth\"\n",
        "os.makedirs(os.path.dirname(save_path), exist_ok=True)\n",
        "torch.save(model.state_dict(), save_path)"
      ],
      "metadata": {
        "id": "TEirKKrsqGKg"
      },
      "id": "TEirKKrsqGKg",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}